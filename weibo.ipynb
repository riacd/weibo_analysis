{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===开始爬取第2页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:千万不要长期喝奶茶.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:千万不要长期喝奶茶.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:千万不要长期喝奶茶.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:千万不要长期喝奶茶.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:千万不要长期喝奶茶.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:微信新增锁定功能.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:微信新增锁定功能.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:微信新增锁定功能.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "9\n",
      "text_list is:\n",
      "id_list: 9\n",
      "9\n",
      "region_name_list: 9\n",
      "9\n",
      "9\n",
      "9\n",
      "csv保存成功:微信新增锁定功能.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:微信新增锁定功能.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:坐高铁被窗外的中国基建所惊艳.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:坐高铁被窗外的中国基建所惊艳.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:坐高铁被窗外的中国基建所惊艳.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:坐高铁被窗外的中国基建所惊艳.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:坐高铁被窗外的中国基建所惊艳.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:宋茜好辣.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:宋茜好辣.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "9\n",
      "text_list is:\n",
      "id_list: 9\n",
      "9\n",
      "region_name_list: 9\n",
      "9\n",
      "9\n",
      "9\n",
      "csv保存成功:宋茜好辣.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:宋茜好辣.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "9\n",
      "text_list is:\n",
      "id_list: 9\n",
      "9\n",
      "region_name_list: 9\n",
      "9\n",
      "9\n",
      "9\n",
      "csv保存成功:宋茜好辣.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:24岁研究生离世捐器官救多人.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:24岁研究生离世捐器官救多人.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:24岁研究生离世捐器官救多人.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:24岁研究生离世捐器官救多人.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:24岁研究生离世捐器官救多人.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:罗云熙不上综艺的原因.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:罗云熙不上综艺的原因.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:罗云熙不上综艺的原因.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:罗云熙不上综艺的原因.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:罗云熙不上综艺的原因.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "0\n",
      "text_list is:\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "9\n",
      "text_list is:\n",
      "id_list: 9\n",
      "9\n",
      "region_name_list: 9\n",
      "9\n",
      "9\n",
      "9\n",
      "csv保存成功:刘亚仁被人泼水.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:刘亚仁被人泼水.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:刘亚仁被人泼水.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:刘亚仁被人泼水.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:易烊千玺是在撒娇吗.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:易烊千玺是在撒娇吗.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:易烊千玺是在撒娇吗.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:易烊千玺是在撒娇吗.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "9\n",
      "text_list is:\n",
      "id_list: 9\n",
      "9\n",
      "region_name_list: 9\n",
      "9\n",
      "9\n",
      "9\n",
      "csv保存成功:易烊千玺是在撒娇吗.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "9\n",
      "text_list is:\n",
      "id_list: 9\n",
      "9\n",
      "region_name_list: 9\n",
      "9\n",
      "9\n",
      "9\n",
      "csv保存成功:员工每天带薪拉屎3至6小时被解雇.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:员工每天带薪拉屎3至6小时被解雇.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:员工每天带薪拉屎3至6小时被解雇.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:员工每天带薪拉屎3至6小时被解雇.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:员工每天带薪拉屎3至6小时被解雇.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "9\n",
      "text_list is:\n",
      "id_list: 9\n",
      "9\n",
      "region_name_list: 9\n",
      "9\n",
      "9\n",
      "9\n",
      "csv保存成功:护心十五集前后不是一个编剧.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:护心十五集前后不是一个编剧.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:护心十五集前后不是一个编剧.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:护心十五集前后不是一个编剧.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:护心十五集前后不是一个编剧.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:都不买榴莲我就放心了.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:都不买榴莲我就放心了.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:都不买榴莲我就放心了.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:都不买榴莲我就放心了.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:都不买榴莲我就放心了.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:杨紫表情管理.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:杨紫表情管理.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:杨紫表情管理.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:杨紫表情管理.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:杨紫表情管理.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:谁给白鹿做的妆造.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:谁给白鹿做的妆造.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:谁给白鹿做的妆造.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:谁给白鹿做的妆造.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:谁给白鹿做的妆造.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:催收巨头湖南永雄宣布停业.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:催收巨头湖南永雄宣布停业.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:催收巨头湖南永雄宣布停业.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "9\n",
      "text_list is:\n",
      "id_list: 9\n",
      "9\n",
      "region_name_list: 9\n",
      "9\n",
      "9\n",
      "9\n",
      "csv保存成功:催收巨头湖南永雄宣布停业.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:催收巨头湖南永雄宣布停业.csv\n",
      "数据清洗完成\n",
      "===开始爬取第2页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:金玟庭的背.csv\n",
      "===开始爬取第3页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:金玟庭的背.csv\n",
      "===开始爬取第4页微博===\n",
      "200\n",
      "9\n",
      "text_list is:\n",
      "id_list: 9\n",
      "9\n",
      "region_name_list: 9\n",
      "9\n",
      "9\n",
      "9\n",
      "csv保存成功:金玟庭的背.csv\n",
      "===开始爬取第5页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:金玟庭的背.csv\n",
      "===开始爬取第6页微博===\n",
      "200\n",
      "10\n",
      "text_list is:\n",
      "id_list: 10\n",
      "10\n",
      "region_name_list: 10\n",
      "10\n",
      "10\n",
      "10\n",
      "csv保存成功:金玟庭的背.csv\n",
      "数据清洗完成\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from jsonpath import jsonpath  # 解析json数据\n",
    "import pandas as pd  # 存取csv文件\n",
    "import datetime  # 转换时间用\n",
    "import numpy as np\n",
    "\n",
    "def get_topics():\n",
    "    url = r'https://www.weibo.cn/'\n",
    "    headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36 Edg/113.0.1774.42\"}\n",
    "    r = requests.get(url, headers = headers)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    raw_topics = re.findall(r'>\\#.+?\\#</a>', str(soup))\n",
    "    topics = []\n",
    "    for i, v in enumerate(raw_topics):\n",
    "        topics.append(v[2:-5])\n",
    "    return topics\n",
    "\n",
    "def get_comments(keyword, max_page, time):\n",
    "    \"\"\"\n",
    "    爬取微博内容列表\n",
    "    :param keyword: 搜索关键字\n",
    "    :param max_page: 爬取几页\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    def trans_time(v_str):\n",
    "        \"\"\"转换GMT时间为标准格式\"\"\"\n",
    "        GMT_FORMAT = '%a %b %d %H:%M:%S +0800 %Y'\n",
    "        timeArray = datetime.datetime.strptime(v_str, GMT_FORMAT)\n",
    "        ret_time = timeArray.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        return ret_time\n",
    "\n",
    "    def getLongText(v_id, headers):\n",
    "        \"\"\"爬取长微博全文\"\"\"\n",
    "        url = 'https://m.weibo.cn/statuses/extend?id=' + str(v_id)\n",
    "        r = requests.get(url, headers=headers)\n",
    "        json_data = r.json()\n",
    "        long_text = json_data['data']['longTextContent']\n",
    "        # 微博内容-正则表达式数据清洗\n",
    "        dr = re.compile(r'<[^>]+>', re.S)\n",
    "        long_text2 = dr.sub('', long_text)\n",
    "        # print(long_text2)\n",
    "        return long_text2\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Mobile Safari/537.36\",\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "        \"accept-encoding\": \"gzip, deflate, br\",\n",
    "    }\n",
    "\n",
    "    file_name = '{}.csv'.format(keyword)\n",
    "\n",
    "    path = time + '/' + file_name\n",
    "\n",
    "    if not os.path.exists(time):\n",
    "        os.mkdir(time)\n",
    "\n",
    "    # # 如果csv文件存在，先删除之\n",
    "    # if os.path.exists(file_name):\n",
    "    #     os.remove(file_name)\n",
    "    #     print('微博清单存在，已删除: {}'.format(file_name))\n",
    "\n",
    "    for page in range(2, max_page + 2):\n",
    "        print('===开始爬取第{}页微博==='.format(page))\n",
    "        # 请求地址\n",
    "        url = 'https://m.weibo.cn/api/container/getIndex'\n",
    "        # 请求参数\n",
    "        params = {\n",
    "            \"containerid\": \"100103type=1&q={}\".format(keyword),\n",
    "            \"page_type\": \"searchall\",\n",
    "            \"page\": page\n",
    "        }\n",
    "        # 发送请求\n",
    "        r = requests.get(url, headers=headers, params=params)\n",
    "        print(r.status_code)\n",
    "        # pprint(r.json())\n",
    "        # 解析json数据\n",
    "        cards = r.json()[\"data\"][\"cards\"]\n",
    "        print(len(cards))\n",
    "        region_name_list = []\n",
    "        status_city_list = []\n",
    "        status_province_list = []\n",
    "        status_country_list = []\n",
    "        for card in cards:\n",
    "            # 发布于\n",
    "            try:\n",
    "                region_name = card['card_group'][0]['mblog']['region_name']\n",
    "                region_name_list.append(region_name)\n",
    "            except:\n",
    "                region_name_list.append('')\n",
    "            # ip属地_城市\n",
    "            try:\n",
    "                status_city = card['card_group'][0]['mblog']['status_city']\n",
    "                status_city_list.append(status_city)\n",
    "            except:\n",
    "                status_city_list.append('')\n",
    "            # ip属地_省份\n",
    "            try:\n",
    "                status_province = card['card_group'][0]['mblog']['status_province']\n",
    "                status_province_list.append(status_province)\n",
    "            except:\n",
    "                status_province_list.append('')\n",
    "            # ip属地_国家\n",
    "            try:\n",
    "                status_country = card['card_group'][0]['mblog']['status_country']\n",
    "                status_country_list.append(status_country)\n",
    "            except:\n",
    "                status_country_list.append('')\n",
    "        # 微博内容\n",
    "        text_list = jsonpath(cards, '$..mblog.text')\n",
    "        # 微博内容-正则表达式数据清洗\n",
    "        dr = re.compile(r'<[^>]+>', re.S)\n",
    "        text2_list = []\n",
    "        print('text_list is:')\n",
    "        # print(text_list)\n",
    "        if not text_list:  # 如果未获取到微博内容，进入下一轮循环\n",
    "            continue\n",
    "        if type(text_list) == list and len(text_list) > 0:\n",
    "            for text in text_list:\n",
    "                text2 = dr.sub('', text)  # 正则表达式提取微博内容\n",
    "                # print(text2)\n",
    "                text2_list.append(text2)\n",
    "        # 微博创建时间\n",
    "        time_list = jsonpath(cards, '$..mblog.created_at')\n",
    "        time_list = [trans_time(v_str=i) for i in time_list]\n",
    "        # 微博作者\n",
    "        author_list = jsonpath(cards, '$..mblog.user.screen_name')\n",
    "        # 微博id\n",
    "        id_list = jsonpath(cards, '$..mblog.id')\n",
    "        # 判断是否存在全文\n",
    "        isLongText_list = jsonpath(cards, '$..mblog.isLongText')\n",
    "        idx = 0\n",
    "        for i in isLongText_list:\n",
    "            if i == True:\n",
    "                long_text = getLongText(id_list[idx], headers)\n",
    "                text2_list[idx] = long_text\n",
    "            idx += 1\n",
    "        # 转发数\n",
    "        reposts_count_list = jsonpath(cards, '$..mblog.reposts_count')\n",
    "        # 评论数\n",
    "        comments_count_list = jsonpath(cards, '$..mblog.comments_count')\n",
    "        # 点赞数\n",
    "        attitudes_count_list = jsonpath(cards, '$..mblog.attitudes_count')\n",
    "        # 把列表数据保存成DataFrame数据\n",
    "        print('id_list:', len(id_list))\n",
    "        print(len(time_list))\n",
    "        print('region_name_list:', len(region_name_list))\n",
    "        print(len(status_city_list))\n",
    "        print(len(status_province_list))\n",
    "        print(len(status_country_list))\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                '页码': [page] * len(id_list),\n",
    "                '微博id': id_list,\n",
    "                '微博作者': author_list,\n",
    "                '发布时间': time_list,\n",
    "                '微博内容': text2_list,\n",
    "                '转发数': reposts_count_list,\n",
    "                '评论数': comments_count_list,\n",
    "                '点赞数': attitudes_count_list,\n",
    "                '发布于': region_name_list,\n",
    "                'ip属地_城市': status_city_list,\n",
    "                'ip属地_省份': status_province_list,\n",
    "                'ip属地_国家': status_country_list,\n",
    "            }\n",
    "        )\n",
    "        # 表头\n",
    "        if os.path.exists(path):\n",
    "            header = None\n",
    "        else:\n",
    "            header = ['页码', '微博id', '微博作者', '发布时间', '微博内容', '转发数', '评论数',\n",
    "                      '点赞数', '发布于', 'ip属地_城市', 'ip属地_省份', 'ip属地_国家']  # csv文件头\n",
    "        # 保存到csv文件\n",
    "\n",
    "        df.to_csv(path, mode='a+', index=False,\n",
    "                  header=header, encoding='utf_8_sig')\n",
    "        print('csv保存成功:{}'.format(file_name))\n",
    "\n",
    "    # 数据清洗-去重\n",
    "    df = pd.read_csv(path)\n",
    "    # 删除重复数据\n",
    "    df.drop_duplicates(subset=['微博id'], inplace=True, keep='first')\n",
    "    # 再次保存csv文件\n",
    "    df.to_csv(path, index=False, encoding='utf_8_sig')\n",
    "    print('数据清洗完成')\n",
    "\n",
    "def mainloop():\n",
    "    while True:\n",
    "        # 话题简单跟踪\n",
    "        file_name = '话题跟踪.csv'\n",
    "        topics = get_topics()\n",
    "        t = str(datetime.datetime.now())[:-7].replace(':', '-')\n",
    "        topic_list = topics\n",
    "        rank_list = list(np.arange(len(topic_list)))\n",
    "        time_list = [t] * len(topic_list)\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                '话题': topic_list,\n",
    "                '排名': rank_list,\n",
    "                '时刻': time_list,\n",
    "            }\n",
    "        )\n",
    "        if os.path.exists(file_name):\n",
    "            header = None\n",
    "        else:\n",
    "            header = ['话题', '排名', '时刻']  # csv文件头\n",
    "        df.to_csv(file_name, mode='a+', index=False, header=header, encoding='utf_8_sig')\n",
    "        \n",
    "        # 话题评论留档\n",
    "        for topic in topics:\n",
    "            get_comments(topic, 5, t)\n",
    "            time.sleep(1) # 稍微有点怕被反爬\n",
    "        \n",
    "        # 休息一会（微博热榜每十分钟刷新一次）\n",
    "        time.sleep(600)\n",
    "\n",
    "mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
